{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d87b94c9-ce14-4fdb-98d3-53a4273a253e",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "Work captured here: https://github.com/cytora/cytora-llm-testing/issues/8\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "030f2b24-25c9-40d4-8314-315b68294eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"EleutherAI/pythia-12b\"\n",
    "\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "\n",
    "seed = 42\n",
    "gradient_checkpointing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2271e721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['43GB', '45GB']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_free_memory():\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    mem_info = torch.cuda.mem_get_info()\n",
    "    max_mem_all_gpus = []\n",
    "    for i in range(n_gpus):\n",
    "        free_in_GB = int(mem_info[i] / 1024**3)\n",
    "        max_memory = f\"{free_in_GB-2}GB\"\n",
    "        max_mem_all_gpus.append(max_memory)\n",
    "    return max_mem_all_gpus\n",
    "\n",
    "\n",
    "get_free_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1936fc97-a71c-47fe-b1e8-9e8e6fd7cda5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f429c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483f86b4-bb2f-45e9-89f1-47914c9ca8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# huggingface_token = os.getenv(\"HF_TOKEN\")\n",
    "# !huggingface-cli login --token $huggingface_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8591240c-c3fb-4874-84fb-c5dc3e9aba5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"sh.command\").setLevel(logging.ERROR)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6294b4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "databricks_consts.py already downloaded\n"
     ]
    }
   ],
   "source": [
    "![ -e databricks_consts.py ] && echo \"databricks_consts.py already downloaded\" ||  wget -O databricks_consts.py https://raw.githubusercontent.com/databrickslabs/dolly/master/training/consts.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "111cd6ca-3a9c-49eb-9ce4-9e67fa7a28c7",
   "metadata": {},
   "source": [
    "# Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "513ec034-ee21-409b-af16-c2bfc26c2c56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/opyate/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-13 22:45:41 INFO [__main__] Loading tokenizer for EleutherAI/pythia-12b\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "from databricks_consts import END_KEY, INSTRUCTION_KEY, RESPONSE_KEY_NL\n",
    "\n",
    "def load_tokenizer(pretrained_model_name_or_path: str) -> PreTrainedTokenizer:\n",
    "    logger.info(f\"Loading tokenizer for {pretrained_model_name_or_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": [END_KEY, INSTRUCTION_KEY, RESPONSE_KEY_NL]})\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer(model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b2fc433-8cff-448f-b3dd-8f6104079dc3",
   "metadata": {},
   "source": [
    "# Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a146b2a8-ce6a-4a07-9370-f78473849785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 22:45:41 INFO [torch.distributed.nn.jit.instantiator] Created a temporary directory at /tmp/tmpers2ns4x\n",
      "2023-06-13 22:45:41 INFO [torch.distributed.nn.jit.instantiator] Writing /tmp/tmpers2ns4x/_remote_module_non_scriptable.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/opyate/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/opyate/anaconda3/envs/pythia-dolly-lora-py39 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/opyate/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/share/gconf/ubuntu.default.path')}\n",
      "  warn(msg)\n",
      "/home/opyate/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/org/gnome/Terminal/screen/223dac04_71fe_4ef2_aad8_6706330d659a')}\n",
      "  warn(msg)\n",
      "/home/opyate/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('0'), PosixPath('1')}\n",
      "  warn(msg)\n",
      "/home/opyate/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/share/gconf/ubuntu.mandatory.path')}\n",
      "  warn(msg)\n",
      "/home/opyate/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('local/nil'), PosixPath('@/tmp/.ICE-unix/3441,unix/nil')}\n",
      "  warn(msg)\n",
      "/home/opyate/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/opyate/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/xdg/xdg-ubuntu')}\n",
      "  warn(msg)\n",
      "/home/opyate/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/opyate/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/opyate/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/opyate/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 22:45:42 INFO [__main__] Loading model for EleutherAI/pythia-12b\n",
      "2023-06-13 22:45:43 WARNING [accelerate.utils.modeling] The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,898,240 || all params: 11,851,970,560 || trainable%: 0.04976590154473013\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    pretrained_model_name_or_path: str, *, gradient_checkpointing: bool = False\n",
    ") -> AutoModelForCausalLM:\n",
    "    logger.info(f\"Loading model for {pretrained_model_name_or_path}\")\n",
    "\n",
    "    # not QLoRA yet - get regular LoRA working first!\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        use_cache=not gradient_checkpointing,\n",
    "        device_map=\"auto\",\n",
    "        # quantization_config=bnb_config,  # for QLoRA\n",
    "    )\n",
    "\n",
    "    # maybe enable this?\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=gradient_checkpointing)\n",
    "\n",
    "    # The dimension used by the LoRA update matrices\n",
    "    LORA_R = 8\n",
    "    # Scaling factor\n",
    "    LORA_ALPHA = 32\n",
    "    LORA_DROPOUT = 0.05\n",
    "\n",
    "    # r and alpha together control the total number of final trainable parameters when using LoRA, giving you the flexibility to balance a trade-off between end performance and compute efficiency.\n",
    "    config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",  # Specifies if the bias parameters should be trained\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=[\"query_key_value\"],\n",
    "        # inference_mode=False  # default is False\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    if gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = load_model(model_id, gradient_checkpointing=gradient_checkpointing)\n",
    "\n",
    "# If you see this error below:\n",
    "# UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files\n",
    "# Ignore it - bitsandbytes just doesn't know about Ubuntu's alternatives system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e518d7d-b2f5-43ed-9f4a-cca7580691b1",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffa54077-b615-44fd-b9a5-1a7da507d578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from databricks_consts import PROMPT_WITH_INPUT_FORMAT, PROMPT_NO_INPUT_FORMAT\n",
    "from functools import partial\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocess_batch(batch: Dict[str, List], tokenizer: AutoTokenizer, max_length: int) -> dict:\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_training_dataset(path_or_dataset) -> Dataset:\n",
    "    logger.info(f\"Loading dataset: {path_or_dataset}\")\n",
    "    dataset = load_dataset(path_or_dataset)[\"train\"]\n",
    "    \n",
    "    logger.info(\"Found %d rows\", dataset.num_rows)\n",
    "\n",
    "    def _add_text(rec):\n",
    "        instruction = rec[\"instruction\"]\n",
    "        response = rec[\"response\"]\n",
    "        context = rec.get(\"context\")\n",
    "\n",
    "        if not instruction:\n",
    "            raise ValueError(f\"Expected an instruction in: {rec}\")\n",
    "\n",
    "        if not response:\n",
    "            raise ValueError(f\"Expected a response in: {rec}\")\n",
    "\n",
    "        # For some instructions there is an input that goes along with the instruction, providing context for the\n",
    "        # instruction.  For example, the input might be a passage from Wikipedia and the instruction says to extract\n",
    "        # some piece of information from it.  The response is that information to extract.  In other cases there is\n",
    "        # no input.  For example, the instruction might be open QA such as asking what year some historic figure was\n",
    "        # born.\n",
    "        if context:\n",
    "            rec[\"text\"] = PROMPT_WITH_INPUT_FORMAT.format(instruction=instruction, response=response, input=context)\n",
    "        else:\n",
    "            rec[\"text\"] = PROMPT_NO_INPUT_FORMAT.format(instruction=instruction, response=response)\n",
    "        return rec\n",
    "\n",
    "    dataset = dataset.map(_add_text)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, training_dataset: str, seed=42) -> Dataset:\n",
    "    \"\"\"Loads the training dataset and tokenizes it so it is ready for training.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (AutoTokenizer): Tokenizer tied to the model.\n",
    "        max_length (int): Maximum number of tokens to emit from tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: HuggingFace dataset\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = load_training_dataset(training_dataset)\n",
    "\n",
    "    logger.info(\"Preprocessing dataset\")\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
    "    )\n",
    "\n",
    "    # Make sure we don't have any truncated records, as this would mean the end keyword is missing.\n",
    "    logger.info(\"Processed dataset has %d rows\", dataset.num_rows)\n",
    "    dataset = dataset.filter(lambda rec: len(rec[\"input_ids\"]) < max_length)\n",
    "    logger.info(\"Processed dataset has %d rows after filtering for truncated records\", dataset.num_rows)\n",
    "\n",
    "    logger.info(\"Shuffling dataset\")\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    logger.info(\"Done preprocessing\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class DataCollatorForCompletionOnlyLM(DataCollatorForLanguageModeling):\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        # The prompt ends with the response key plus a newline.  We encode this and then try to find it in the\n",
    "        # sequence of tokens.  This should just be a single token.\n",
    "        response_token_ids = self.tokenizer.encode(RESPONSE_KEY_NL)\n",
    "\n",
    "        labels = batch[\"labels\"].clone()\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "\n",
    "            response_token_ids_start_idx = None\n",
    "            for idx in np.where(batch[\"labels\"][i] == response_token_ids[0])[0]:\n",
    "                response_token_ids_start_idx = idx\n",
    "                break\n",
    "\n",
    "            if response_token_ids_start_idx is None:\n",
    "                raise RuntimeError(\n",
    "                    f'Could not find response key {response_token_ids} in token IDs {batch[\"labels\"][i]}'\n",
    "                )\n",
    "\n",
    "            response_token_ids_end_idx = response_token_ids_start_idx + 1\n",
    "\n",
    "            # Make pytorch loss function ignore all tokens up through the end of the response key\n",
    "            labels[i, :response_token_ids_end_idx] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97dde7de-2463-4a5b-a882-3f1323a10d42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 22:46:16 INFO [__main__] Found max lenth: 2048\n",
      "2023-06-13 22:46:16 INFO [__main__] Loading dataset: databricks/databricks-dolly-15k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 22:46:17 WARNING [datasets.builder] Found cached dataset json (/home/opyate/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 972.03it/s]\n",
      "2023-06-13 22:46:17 INFO [__main__] Found 15011 rows\n",
      "2023-06-13 22:46:18 WARNING [datasets.arrow_dataset] Loading cached processed dataset at /home/opyate/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-2241a2b014caa6d5.arrow\n",
      "2023-06-13 22:46:18 INFO [__main__] Preprocessing dataset\n",
      "2023-06-13 22:46:18 WARNING [datasets.arrow_dataset] Loading cached processed dataset at /home/opyate/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-9bb79cdaaafc4eca.arrow\n",
      "2023-06-13 22:46:18 INFO [__main__] Processed dataset has 15011 rows\n",
      "2023-06-13 22:46:18 WARNING [datasets.arrow_dataset] Loading cached processed dataset at /home/opyate/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-8e5865e5b3abe79a.arrow\n",
      "2023-06-13 22:46:18 INFO [__main__] Processed dataset has 14977 rows after filtering for truncated records\n",
      "2023-06-13 22:46:18 INFO [__main__] Shuffling dataset\n",
      "2023-06-13 22:46:18 WARNING [datasets.arrow_dataset] Loading cached shuffled indices for dataset at /home/opyate/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-1b72eb692ce09050.arrow\n",
      "2023-06-13 22:46:18 INFO [__main__] Done preprocessing\n",
      "2023-06-13 22:46:18 WARNING [datasets.arrow_dataset] Loading cached split indices for dataset at /home/opyate/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-4aa2c0320d490a7f.arrow and /home/opyate/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-ae9e0200b458215d.arrow\n",
      "2023-06-13 22:46:18 INFO [__main__] Train data size: 11982\n",
      "2023-06-13 22:46:18 INFO [__main__] Test data size: 2995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_size: 2995\n"
     ]
    }
   ],
   "source": [
    "max_length = None\n",
    "for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "    max_length = getattr(model.config, length_setting, None)\n",
    "    if max_length:\n",
    "        logger.info(f\"Found max lenth: {max_length}\")\n",
    "        break\n",
    "if not max_length:\n",
    "    max_length = 1024\n",
    "    logger.info(f\"Using default max length: {max_length}\")\n",
    "\n",
    "print(f\"max_length: {max_length}\")\n",
    "\n",
    "processed_dataset = preprocess_dataset(tokenizer=tokenizer, max_length=max_length, seed=seed, training_dataset=dataset_name)\n",
    "\n",
    "test_size = int(len(processed_dataset[\"input_ids\"]) * 0.2)\n",
    "print(f\"test_size: {test_size}\")\n",
    "\n",
    "split_dataset = processed_dataset.train_test_split(test_size=test_size, seed=seed)\n",
    "\n",
    "logger.info(\"Train data size: %d\", split_dataset[\"train\"].num_rows)\n",
    "logger.info(\"Test data size: %d\", split_dataset[\"test\"].num_rows)\n",
    "\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer=tokenizer, mlm=False, return_tensors=\"pt\", pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01ebbb0c-9325-47c3-b81b-3a9311cd9084",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4ceabe-221e-4335-8841-0a7e4b5fcebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "base_folder = f\"dolly_training/dolly_{timestamp}\"\n",
    "\n",
    "# ensure base_folder exists\n",
    "pathlib.Path(base_folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "875662e0-b045-4795-87c7-9b8e476fa161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5990 [00:00<?, ?it/s]You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|          | 10/5990 [00:09<1:39:42,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.744, 'learning_rate': 4.993319973279894e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/5990 [00:18<1:23:57,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6911, 'learning_rate': 4.98496993987976e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 30/5990 [00:26<1:22:55,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1721, 'learning_rate': 4.976619906479626e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 40/5990 [00:35<1:23:02,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1695, 'learning_rate': 4.968269873079493e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 47.50 GiB total capacity; 45.60 GiB already allocated; 34.38 MiB free; 45.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 59\u001b[0m\n\u001b[1;32m     16\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m     17\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mbase_folder\u001b[39m}\u001b[39;00m\u001b[39m/output_dir\u001b[39m\u001b[39m\"\u001b[39m,  \u001b[39m# see below\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     gradient_accumulation_steps\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     learning_rate\u001b[39m=\u001b[39mlr,\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     51\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     52\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m     data_collator\u001b[39m=\u001b[39mDataCollatorForCompletionOnlyLM(tokenizer, mlm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m     57\u001b[0m )\n\u001b[0;32m---> 59\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/transformers/trainer.py:1545\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1542\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1543\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1544\u001b[0m )\n\u001b[0;32m-> 1545\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1546\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1547\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1548\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1549\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1550\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/transformers/trainer.py:1819\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1816\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1818\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1819\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1821\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1822\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1823\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1824\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1825\u001b[0m ):\n\u001b[1;32m   1826\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1827\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/transformers/trainer.py:2654\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2651\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2653\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2654\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2656\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2657\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/transformers/trainer.py:2679\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2678\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2679\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2680\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/accelerate/utils/operations.py:576\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 576\u001b[0m     \u001b[39mreturn\u001b[39;00m model_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/accelerate/utils/operations.py:564\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 564\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_to_fp32(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/torch/amp/autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 14\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/peft/peft_model.py:739\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m peft_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_peft_config\n\u001b[1;32m    738\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[0;32m--> 739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(\n\u001b[1;32m    740\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    741\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    742\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    743\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m    744\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    745\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    746\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    747\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    748\u001b[0m     )\n\u001b[1;32m    750\u001b[0m batch_size \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    751\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    752\u001b[0m     \u001b[39m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:674\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[39mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[39m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    672\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 674\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpt_neox(\n\u001b[1;32m    675\u001b[0m     input_ids,\n\u001b[1;32m    676\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    677\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    678\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    679\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    680\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    681\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    682\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    683\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    684\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    685\u001b[0m )\n\u001b[1;32m    687\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    688\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_out(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:564\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    556\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    557\u001b[0m         create_custom_forward(layer),\n\u001b[1;32m    558\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    561\u001b[0m         head_mask[i],\n\u001b[1;32m    562\u001b[0m     )\n\u001b[1;32m    563\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 564\u001b[0m     outputs \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    565\u001b[0m         hidden_states,\n\u001b[1;32m    566\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    567\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    568\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    569\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    570\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    571\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    572\u001b[0m     )\n\u001b[1;32m    573\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    574\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:331\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     hidden_states: Optional[torch\u001b[39m.\u001b[39mFloatTensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    330\u001b[0m ):\n\u001b[0;32m--> 331\u001b[0m     attention_layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    332\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layernorm(hidden_states),\n\u001b[1;32m    333\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    334\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    335\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    336\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    337\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    338\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    339\u001b[0m     )\n\u001b[1;32m    340\u001b[0m     attn_output \u001b[39m=\u001b[39m attention_layer_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: attn_output, present, (attn_weights)\u001b[39;00m\n\u001b[1;32m    341\u001b[0m     outputs \u001b[39m=\u001b[39m attention_layer_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:163\u001b[0m, in \u001b[0;36mGPTNeoXAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, layer_past, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    160\u001b[0m present \u001b[39m=\u001b[39m (key, value) \u001b[39mif\u001b[39;00m use_cache \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m# Compute attention\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    165\u001b[0m \u001b[39m# Reshape outputs\u001b[39;00m\n\u001b[1;32m    166\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/pythia-dolly-lora-py39/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:243\u001b[0m, in \u001b[0;36mGPTNeoXAttention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m*\u001b[39m head_mask\n\u001b[0;32m--> 243\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(attn_weights, value)\n\u001b[1;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m attn_output, attn_weights\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 47.50 GiB total capacity; 45.60 GiB already allocated; 34.38 MiB free; 45.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "bf16 = True  # RTX\n",
    "save_steps = 200  # from dolly databricks notebook\n",
    "save_total_limit = 20  # from dolly databricks notebook\n",
    "logging_steps = 10  # from dolly databricks notebook\n",
    "eval_steps = 50  # from dolly databricks notebook\n",
    "epochs = 2\n",
    "logging_steps = 10  # dolly @click default\n",
    "per_device_train_batch_size = 1  # OK for A10 GPUs\n",
    "per_device_eval_batch_size = per_device_train_batch_size\n",
    "lr = 5e-6  # from dolly databricks notebook\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{base_folder}/output_dir\",  # see below\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=2,\n",
    "    # max_steps=10,  # replaced with num_train_epochs ???\n",
    "    \n",
    "    # logging_steps=1,  # see below\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    \n",
    "    # copied from dolly\n",
    "    remove_unused_columns=False,  # https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.remove_unused_columns\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=False,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    \n",
    "    logging_dir=f\"{base_folder}/runs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=logging_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    \n",
    "    num_train_epochs=epochs,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    \n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    \n",
    "    fp16=False,\n",
    "    bf16=bf16,\n",
    "    learning_rate=lr,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    "    data_collator=DataCollatorForCompletionOnlyLM(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "725612e2",
   "metadata": {},
   "source": [
    "I'm getting OutOfMemoryError at this point, so will convert this to a script which I can launch with `accelerate`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dc23c3d-afc0-4577-a2f7-e51193548afc",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f171c4-3ab7-4e91-9995-bd260412c213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir=f\"{base_folder}/model_output_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c49efd-604f-49e0-a319-04d6434ff3ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_name_simple = os.path.basename(dataset_name)\n",
    "model_size = os.path.basename(model_id).split(\"-\")[2]\n",
    "\n",
    "hf_model_name = f\"{dataset_name_simple}_lora_{model_size}_ep{epochs}_lr{lr}_batch{per_device_train_batch_size}\"\n",
    "hf_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c46d57-7628-4fb6-aee4-d4e9e83fe382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# private first, so we can test\n",
    "model.push_to_hub(f\"opyate/{hf_model_name}\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68907518-5e8b-4ff4-91a7-60540222eee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -la $base_folder/model_output_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
